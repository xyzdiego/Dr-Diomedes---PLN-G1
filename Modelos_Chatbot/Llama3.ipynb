{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b966278a0d841f2a9f104dd449de750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_465880e3e4a744a495b4a028a441884f",
              "IPY_MODEL_e1961e76a71044eaacec89be7a4bc4b7",
              "IPY_MODEL_bc028f6aaaa744dfb8954e2c1d8c7e44"
            ],
            "layout": "IPY_MODEL_da3b1407f6564bbfb129c0f272d4e343"
          }
        },
        "465880e3e4a744a495b4a028a441884f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cb653fd06c54c1f8f7d5cbf4048091b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e5c527dd97274979bec663249fb51c2d",
            "value": "Map‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "e1961e76a71044eaacec89be7a4bc4b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5877fe2a65cc48e78d6c6089e21b7940",
            "max": 1816,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2fd53796a5fc470ba0f1007574926a75",
            "value": 1816
          }
        },
        "bc028f6aaaa744dfb8954e2c1d8c7e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b99c94a43a0a4de8aa6e34ac6a30abf3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_be7d2cb30e7d49eebc01842e6f352bfb",
            "value": "‚Äá1816/1816‚Äá[00:05&lt;00:00,‚Äá432.42‚Äáexamples/s]"
          }
        },
        "da3b1407f6564bbfb129c0f272d4e343": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cb653fd06c54c1f8f7d5cbf4048091b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c527dd97274979bec663249fb51c2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5877fe2a65cc48e78d6c6089e21b7940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fd53796a5fc470ba0f1007574926a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b99c94a43a0a4de8aa6e34ac6a30abf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7d2cb30e7d49eebc01842e6f352bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlqJkKJbtpX8",
        "outputId": "a2905227-9aa5-45d4-c335-e1af657a2240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Eres Cantor, un asistente po√©tico y emocional. Responde inspir√°ndote en esta canci√≥n. H√°blame con el sentimiento de la canci√≥n '26 de mayo'.\n",
            "\n",
            "### Response:\n",
            "el veintiseis del mes de mayo nacio un ni√±ito en el a√±o cincuenta y siete y alla en la junta fue bautizado y hoy se conoce con el nombre de diomedes.<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "# REPARACI√ìN DE DATOS\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Cargar Excel\n",
        "df = pd.read_excel('1_dataset_chatbot.xlsx')\n",
        "\n",
        "# Crear una columna con el formato que Llama-3 entiende (Alpaca/Instruct)\n",
        "\n",
        "system_prompt = \"Eres Cantor, un asistente po√©tico y emocional. Responde inspir√°ndote en esta canci√≥n.\"\n",
        "\n",
        "def format_data(row):\n",
        "    # Simulamos una interacci√≥n.\n",
        "    # Entrada: El usuario pide consejo o habla de un tema.\n",
        "    # Salida: El verso de la canci√≥n.\n",
        "\n",
        "    # NOTA: Para un fine-tuning real de alta calidad, idealmente tendr√≠as preguntas variadas.\n",
        "    # Aqu√≠ usaremos un truco: asumimos que el usuario pregunta sobre el tema de la canci√≥n.\n",
        "\n",
        "    instruction = f\"H√°blame con el sentimiento de la canci√≥n '{row['nombre_cancion']}'.\"\n",
        "    output = str(row['estrofa']) # Aseguramos que sea string\n",
        "\n",
        "    # Formato Alpaca (est√°ndar para ense√±ar instrucciones)\n",
        "    text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{system_prompt} {instruction}\n",
        "\n",
        "### Response:\n",
        "{output}<|end_of_text|>\"\"\"\n",
        "\n",
        "    return text\n",
        "\n",
        "# Crear columna de texto formateado\n",
        "df['text'] = df.apply(format_data, axis=1)\n",
        "\n",
        "# Convertir a formato HuggingFace Dataset\n",
        "dataset = Dataset.from_pandas(df[['text']])\n",
        "\n",
        "# Ver un ejemplo de c√≥mo qued√≥\n",
        "print(dataset[0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "\n",
        "# Esta l√≥gica instala la versi√≥n correcta de Unsloth dependiendo de tu GPU en Colab\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # Para GPUs nuevas (A100, H100)\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Para GPUs est√°ndar de Colab (T4 - La que probablemente usas)\n",
        "    !pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "-huB_dEJuYtn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Opci√≥n A: Si guardaste el token en los \"Secretos\" de Colab (la llavecita a la izquierda) con nombre 'HF_TOKEN'\n",
        "try:\n",
        "    login(token=userdata.get('HF_TOKEN'))\n",
        "except:\n",
        "    # Opci√≥n B: Login manual (te pedir√° pegar el token en una cajita)\n",
        "    login()"
      ],
      "metadata": {
        "id": "W_dmzFVgIuyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "print(\"Cargando modelo base... (esto puede tardar unos minutos la primera vez)\")\n",
        "\n",
        "try:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    print(\"¬°Modelo cargado exitosamente!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2A7XBDzIyq2",
        "outputId": "18c18c23-2579-42f8-bb94-dcbae8ba807e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "Unsloth: Could not find Config class in trl.trainer.dpo_trainer. Found: []\n",
            "Unsloth: Could not find Config class in trl.trainer.iterative_sft_trainer. Found: []\n",
            "Unsloth: Could not find Config class in trl.trainer.sft_trainer. Found: []\n",
            "Cargando modelo base... (esto puede tardar unos minutos la primera vez)\n",
            "==((====))==  Unsloth 2025.11.6: Fast Llama patching. Transformers: 4.57.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "¬°Modelo cargado exitosamente!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "print(\"Adaptadores PEFT aplicados correctamente.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuS0umLzI19O",
        "outputId": "b882d48a-6040-46ce-8d75-5c773930dc16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.11.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adaptadores PEFT aplicados correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "bvFJWllL4v6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Configuraci√≥n del entrenamiento\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Cambiado a False por seguridad (True a veces da problemas con datasets peque√±os)\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60, # Pon 60 para probar r√°pido, luego sube a 300-500\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # <--- ESTO SOLUCIONA EL AVISO DE WANDB\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Iniciar entrenamiento\n",
        "print(\"Iniciando entrenamiento...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"¬°Entrenamiento finalizado!\")"
      ],
      "metadata": {
        "id": "pXcFecFPuoTg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0b966278a0d841f2a9f104dd449de750",
            "465880e3e4a744a495b4a028a441884f",
            "e1961e76a71044eaacec89be7a4bc4b7",
            "bc028f6aaaa744dfb8954e2c1d8c7e44",
            "da3b1407f6564bbfb129c0f272d4e343",
            "3cb653fd06c54c1f8f7d5cbf4048091b",
            "e5c527dd97274979bec663249fb51c2d",
            "5877fe2a65cc48e78d6c6089e21b7940",
            "2fd53796a5fc470ba0f1007574926a75",
            "b99c94a43a0a4de8aa6e34ac6a30abf3",
            "be7d2cb30e7d49eebc01842e6f352bfb"
          ]
        },
        "outputId": "61eacc2d-feae-458a-d379-0f226edf89f1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/1816 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b966278a0d841f2a9f104dd449de750"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iniciando entrenamiento...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,816 | Num Epochs = 1 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 04:44, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.643000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.598200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.575600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.655200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.410400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.248900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.352100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.304300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.038300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.027400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.915000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.933300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.705200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.602100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.540800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.342200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.373400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.196600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.961000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.849100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.524800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.874000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.982500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.718600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.836600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.594800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.877400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.325900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.706700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.481800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.541400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.150300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.161800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.317600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.552900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.314900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.500200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.187100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.263300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.240200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.190300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>1.483300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.814400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.344500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.602400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.197700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.293700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.376600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.186500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.488500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.442900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.495800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.344300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.283700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.224300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.321800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.372400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.628600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.349000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.298400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¬°Entrenamiento finalizado!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "def hablar_con_cantor(pregunta):\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "    # Cambiamos un poco el prompt para pedir expl√≠citamente una reflexi√≥n\n",
        "    prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Eres Cantor. Responde a esta situaci√≥n con una reflexi√≥n profunda y varios versos de tus canciones: {pregunta}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens = 256,    # Damos espacio para respuestas largas\n",
        "        min_new_tokens = 50,     # <--- OBLIGAMOS a que escriba al menos 30 \"palabras/tokens\"\n",
        "        repetition_penalty = 1.3, # <--- IMPORTANTE: Evita el \"me siento solo, me siento solo\"\n",
        "        temperature = 0.7,       # Un poco m√°s de creatividad\n",
        "        use_cache = True,\n",
        "    )\n",
        "\n",
        "    respuesta = tokenizer.batch_decode(outputs)[0]\n",
        "    return respuesta.split(\"### Response:\")[-1].replace(\"<|end_of_text|>\", \"\").strip()"
      ],
      "metadata": {
        "id": "eOeESlC8D-CR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(textwrap.fill(hablar_con_cantor(\"¬øQu√© puedo hacer para superar el desamor?\"), width=80))"
      ],
      "metadata": {
        "id": "3Jp0l5pAu21_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c26a22-abca-41c1-8e08-a7365a2649ad"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en la vida no hay que amargarse por algo porque lo bueno puede venir luego del\n",
            "malo pero si es un amor perdido ya nunca mas se podra recuperarlo en su lugar yo\n",
            "me voy despacio sin volverme atr√°s mi corazon fue herido como siempre cuando\n",
            "ella estaba junto al otro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(textwrap.fill(hablar_con_cantor(\"¬øQu√© se necesita para ser feliz\"), width=80))"
      ],
      "metadata": {
        "id": "3ARFUriZDNOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1634a311-bc74-49c3-915d-2054e64504f3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "un amor verdadero sin mentiras ni enga√±os, el cielo es mi hogar pues tengo un\n",
            "buen hermano que me inspirara en la vida y no olvidarlo nunca por eso quiero\n",
            "dedicarme al cantante poeta porque este dia ya llegue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(textwrap.fill(hablar_con_cantor(\"¬øQu√© hacer con la infidelidad?\"), width=80))"
      ],
      "metadata": {
        "id": "RSyw5Q8vENMB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325015ae-1cd5-45ed-d8a9-da7e2e0bac41"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hoy he decidido que no voy a volver m√°s, porque mi amor me ha traicionado como\n",
            "un asesino. Ahora ya estoy desilusionada por esa mujer tan malvadezca que se\n",
            "acost√≥ en el lecho ajeno\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(textwrap.fill(hablar_con_cantor(\"¬øPara qu√© son los amigos?\"), width=80))"
      ],
      "metadata": {
        "id": "ZDwWR0g70eZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90200970-423b-49ff-ccf4-5f558ab9702e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "que sin saberlo el amigo me va ayudando, porque le hago ver la verdad que no hay\n",
            "quien pueda hacerme mal ni desgraciarme. por eso digo yo mi amiga o si prefieres\n",
            "tu hermanita te ayudo en todo lo que necesitas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(textwrap.fill(hablar_con_cantor(\"¬øTomar cerveza¬†da¬†felicidad?\"), width=80))"
      ],
      "metadata": {
        "id": "T3p_u7M10enF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b038966-08f5-47b2-ff74-bd0d23286d28"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "una vez tom√© un trago para que me sentara el dolor pero no se apag√≥ la luz del\n",
            "amor todav√≠a siento esa herida en mi coraz√≥n es como si ella nunca hubiera\n",
            "estado aqu√≠ entonces toma otro tragito porque nos va a emocionarse por dentro\n"
          ]
        }
      ]
    }
  ]
}